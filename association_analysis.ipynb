{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8bd436f",
   "metadata": {},
   "source": [
    "# Association Analysis\n",
    "This notebook contains work done for association analysis on the Mozambique\n",
    "dataset from IPUMS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b642ec1",
   "metadata": {},
   "source": [
    "Don't attempt to run this file, it takes around 24 hours and may encounter memory issues. The final csvs are in the shared folder and I have pickle files of all the intermediate data structures if we need them. - Arlan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1365c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dependencies\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from efficient_apriori import apriori # https://pypi.org/project/efficient-apriori/\n",
    "\n",
    "# Load Custom Scripts\n",
    "from src.utils.ipums_extract import (\n",
    "    load_ipums_from_pkl,\n",
    ")\n",
    "\n",
    "PKL_PATH = Path(r\"data/mozambique.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea82bb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5929529, 66)\n",
      "(4974569, 66)\n"
     ]
    }
   ],
   "source": [
    "# Load from PKL\n",
    "mig1_data, mig5_data = load_ipums_from_pkl(PKL_PATH)\n",
    "\n",
    "print(mig1_data.shape)\n",
    "print(mig5_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f06cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69067, 65)\n",
      "(228173, 65)\n"
     ]
    }
   ],
   "source": [
    "# Only use records where migration is 1 for itemset building, bring back others when calculating rule metrics\n",
    "mig1_data_filtered = mig1_data[mig1_data['MIGRATE1'] == 1].drop('MIGRATE1', axis=1).copy()\n",
    "mig5_data_filtered = mig5_data[mig5_data['MIGRATE5'] == 1].drop('MIGRATE5', axis=1).copy()\n",
    "\n",
    "print(mig1_data_filtered.shape)\n",
    "print(mig5_data_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7f71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the data as nested list of strings for apriori algorithm\n",
    "transactions1 = []\n",
    "for index, row in mig1_data_filtered.iterrows():\n",
    "    transaction_items = tuple(row.index[row == 1].tolist())\n",
    "    transactions1.append(transaction_items)\n",
    "\n",
    "transactions5 = []\n",
    "for index, row in mig5_data_filtered.iterrows():\n",
    "    transaction_items = tuple(row.index[row == 1].tolist())\n",
    "    transactions5.append(transaction_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b9987734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriori for mig1\n",
    "itemsets1, rules1 = apriori(transactions1, min_support=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03018ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriori for mig5 - first part due to memory limitations\n",
    "itemsets5a, rules5a = apriori(transactions5[:114000], min_support=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b22ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriori for mig5 - second part due to memory limitations\n",
    "itemsets5b, rules5b = apriori(transactions5[114000:], min_support=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c2114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile list of rules and metrics for {x} -> MIGRATE1\n",
    "results1 = []\n",
    "col_avg = mig1_data.mean()\n",
    "total_count = len(mig1_data)\n",
    "mig_count = len(mig1_data_filtered)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for itemset_size, list_of_itemsets in itemsets1.items():\n",
    "    for itemset, support in list_of_itemsets.items():\n",
    "        itemset = list(itemset)\n",
    "        if counter % 1000 == 0:\n",
    "            print(counter)\n",
    "        count_itemset = (mig1_data[itemset] == 1).all(axis=1).sum()\n",
    "        confidence = support / count_itemset\n",
    "        lift = confidence / (mig_count / total_count)\n",
    "        interest = (support / mig_count) / (np.prod([col_avg[col] for col in list(itemset)]) * col_avg['MIGRATE1'])\n",
    "\n",
    "        results1.append([itemset, support, confidence, lift, interest])\n",
    "        counter += 1\n",
    "\n",
    "# Write to csv\n",
    "results1_df = pd.DataFrame(results1, columns=['itemset', 'support', 'confidence', 'lift', 'interest'])\n",
    "results1_df.to_csv('mig5_final_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d976ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile list of rules and metrics for {x} -> MIGRATE5\n",
    "# To avoid crashes and memory issues intermediate results are written out every 10k rules\n",
    "results5 = []\n",
    "col_avg = mig5_data.mean()\n",
    "total_count = len(mig5_data)\n",
    "mig_count = len(mig5_data_filtered)\n",
    "\n",
    "setA = {itemset for itemset_size, list_of_itemsets in itemsets5a.items() for itemset, support in list_of_itemsets.items()}\n",
    "setB = {itemset for itemset_size, list_of_itemsets in itemsets5b.items() for itemset, support in list_of_itemsets.items()}\n",
    "itemsets = setA | setB\n",
    "print(len(itemsets))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for itemset in itemsets:\n",
    "    itemset = list(itemset)\n",
    "    if counter % 1000 == 0:\n",
    "        print(counter)\n",
    "\n",
    "    support = (mig5_data_filtered[itemset] == 1).all(axis=1).sum()\n",
    "    count_itemset = (mig5_data[itemset] == 1).all(axis=1).sum()\n",
    "    confidence = support / count_itemset\n",
    "    lift = confidence / (mig_count / total_count)\n",
    "    interest = (support / mig_count) / (np.prod([col_avg[col] for col in list(itemset)]) * col_avg['MIGRATE5'])\n",
    "\n",
    "    results5.append([itemset, support, confidence, lift, interest])\n",
    "    counter += 1\n",
    "\n",
    "    if counter > 0 and counter % 10000 == 0:\n",
    "        results5_df = pd.DataFrame(results5, columns=['itemset', 'support', 'confidence', 'lift', 'interest'])\n",
    "        results5_df.to_csv(f'aa_mig5_portion{counter}.csv')\n",
    "        results5 = []\n",
    "\n",
    "\n",
    "results5_df = pd.DataFrame(results5, columns=['itemset', 'support', 'confidence', 'lift', 'interest'])\n",
    "results5_df.to_csv(f'aa_mig5_portion{counter}.csv')\n",
    "results5 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a72f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully combined 14 CSV files into mig5_final_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "# combine all files into a single csv\n",
    "all_files = [f'aa_mig5_portion{i*10000}.csv' for i in range(1,14)] + ['aa_mig5_portion137134.csv']\n",
    "df_list = []\n",
    "output_csv_path = 'mig5_final_analysis.csv'\n",
    "\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df_list.append(df)\n",
    "\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "combined_df.to_csv(output_csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
