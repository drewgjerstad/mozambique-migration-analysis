{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8bd436f",
   "metadata": {},
   "source": [
    "# Association Analysis\n",
    "This notebook contains work done for association analysis on the Mozambique\n",
    "dataset from IPUMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1365c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dependencies\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from efficient_apriori import apriori # https://pypi.org/project/efficient-apriori/\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Custom Scripts\n",
    "from src.utils.ipums_extract import (\n",
    "    load_ipums_from_pkl,\n",
    ")\n",
    "\n",
    "PKL_PATH = Path(r\"data/mozambique.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea82bb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5929529, 66)\n",
      "(4974569, 66)\n"
     ]
    }
   ],
   "source": [
    "# Load from PKL\n",
    "mig1_df, mig5_df = load_ipums_from_pkl(PKL_PATH)\n",
    "\n",
    "print(mig1_df.shape)\n",
    "print(mig5_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50cb3b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "SEED = 5523\n",
    "CV_RATIO = 0.20\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "TRAIN_RATIO = 1 - VAL_RATIO - TEST_RATIO\n",
    "\n",
    "# Create Development Splits (Train/Val/Test)\n",
    "\n",
    "# Split the data set\n",
    "df1_train, df1_test = train_test_split(mig1_df, test_size=TEST_RATIO, random_state=SEED) \n",
    "df1_train, df1_val = train_test_split(mig1_df, test_size=VAL_RATIO/(1-TEST_RATIO), random_state=SEED)\n",
    "\n",
    "df5_train, df5_test = train_test_split(mig5_df, test_size=TEST_RATIO, random_state=SEED) \n",
    "df5_train, df5_val = train_test_split(mig5_df, test_size=VAL_RATIO/(1-TEST_RATIO), random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "809f06cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56721, 65)\n",
      "(187536, 65)\n"
     ]
    }
   ],
   "source": [
    "# Only use records where migration is 1 for itemset building, bring back others when calculating rule metrics\n",
    "df1_train_filtered = df1_train[df1_train['MIGRATE1'] == 1].drop('MIGRATE1', axis=1).copy()\n",
    "df5_train_filtered = df5_train[df5_train['MIGRATE5'] == 1].drop('MIGRATE5', axis=1).copy()\n",
    "\n",
    "print(df1_train_filtered.shape)\n",
    "print(df5_train_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a7f71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the data as nested list of strings for apriori algorithm\n",
    "transactions1 = []\n",
    "for index, row in df1_train_filtered.iterrows():\n",
    "    transaction_items = tuple(row.index[row == 1].tolist())\n",
    "    transactions1.append(transaction_items)\n",
    "\n",
    "transactions5 = []\n",
    "for index, row in df5_train_filtered.iterrows():\n",
    "    transaction_items = tuple(row.index[row == 1].tolist())\n",
    "    transactions5.append(transaction_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9987734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriori for mig1\n",
    "itemsets1, rules1 = apriori(transactions1, min_support=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03018ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriori for mig5 - first part due to memory limitations\n",
    "itemsets5a, rules5a = apriori(transactions5, min_support=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f11c2114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "# Compile list of rules and metrics for {x} -> MIGRATE1\n",
    "results1 = []\n",
    "col_avg = df1_train.mean()\n",
    "total_count = len(df1_train)\n",
    "mig_count = len(df1_train_filtered)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for itemset_size, list_of_itemsets in itemsets1.items():\n",
    "    for itemset, support in list_of_itemsets.items():\n",
    "        itemset = list(itemset)\n",
    "        if counter % 1000 == 0:\n",
    "            print(counter)\n",
    "        count_itemset = (df1_train[itemset] == 1).all(axis=1).sum()\n",
    "        confidence = support / count_itemset\n",
    "        lift = confidence / (mig_count / total_count)\n",
    "        interest = (support / mig_count) / (np.prod([col_avg[col] for col in list(itemset)]) * col_avg['MIGRATE1'])\n",
    "\n",
    "        results1.append([itemset, support, confidence, lift, interest])\n",
    "        counter += 1\n",
    "\n",
    "# Write to csv\n",
    "results1_df = pd.DataFrame(results1, columns=['itemset', 'support', 'confidence', 'lift', 'interest'])\n",
    "results1_df.to_csv('mig1_final_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d976ce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "# Compile list of rules and metrics for {x} -> MIGRATE5\n",
    "# To avoid crashes and memory issues intermediate results are written out every 10k rules\n",
    "results5 = []\n",
    "col_avg = df5_train.mean()\n",
    "total_count = len(df5_train)\n",
    "mig_count = len(df5_train_filtered)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for itemset_size, list_of_itemsets in itemsets5a.items():\n",
    "    for itemset, support in list_of_itemsets.items():\n",
    "        itemset = list(itemset)\n",
    "        if counter % 1000 == 0:\n",
    "            print(counter)\n",
    "        count_itemset = (df5_train[itemset] == 1).all(axis=1).sum()\n",
    "        confidence = support / count_itemset\n",
    "        lift = confidence / (mig_count / total_count)\n",
    "        interest = (support / mig_count) / (np.prod([col_avg[col] for col in list(itemset)]) * col_avg['MIGRATE5'])\n",
    "\n",
    "        results5.append([itemset, support, confidence, lift, interest])\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "results5_df = pd.DataFrame(results5, columns=['itemset', 'support', 'confidence', 'lift', 'interest'])\n",
    "results5_df.to_csv(f'mig5_final_analysis.csv')\n",
    "results5 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11169b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mig1: (6888, 5)\n",
      "mig5: (6328, 5)\n"
     ]
    }
   ],
   "source": [
    "mig1_rules = pd.read_csv(\"data/mig1_final_analysis.csv\")\n",
    "mig5_rules = pd.read_csv(\"data/mig5_final_analysis.csv\")\n",
    "mig1_rules = mig1_rules.drop(columns=['Unnamed: 0'])\n",
    "mig5_rules = mig5_rules.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "print(f'mig1: {mig1_rules.shape}')\n",
    "print(f'mig5: {mig5_rules.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7faf87b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoheg\\AppData\\Local\\Temp\\ipykernel_33060\\670304634.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  items = str(row[1][0])[1:-1].replace(\"'\", \"\").replace(\",\", \"\").split(\" \")\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy, precision, recall and f1 for each rule in mig1\n",
    "total = df1_val.shape[0]\n",
    "total_mig = sum(df1_val['MIGRATE1'] == 1)\n",
    "\n",
    "acc_list = []\n",
    "prec_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "for row in mig1_rules.iterrows():\n",
    "    items = str(row[1][0])[1:-1].replace(\"'\", \"\").replace(\",\", \"\").split(\" \")\n",
    "    full_items = items + ['MIGRATE1']\n",
    "    TP = np.sum(np.prod([df1_val[col] for col in full_items], axis=0))\n",
    "    total_rule = np.sum(np.prod([df1_val[col] for col in items], axis=0))\n",
    "    FN = total_mig - TP\n",
    "    FP = total_rule - TP\n",
    "    TN = total - TP - FN - FP\n",
    "\n",
    "    acc_list.append((TP + TN) / (total))\n",
    "    prec_list.append(TP / (TP + FP))\n",
    "    recall_list.append(TP / (TP + FN))\n",
    "    f1_list.append((2 * TP) / (2 * TP + FP + FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c596e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mig1_rules['accuracy'] = acc_list\n",
    "mig1_rules['precision'] = prec_list\n",
    "mig1_rules['recall'] = recall_list\n",
    "mig1_rules['f1_score'] = f1_list\n",
    "mig1_rules.to_csv(f'mig1_final_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f27e6752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoheg\\AppData\\Local\\Temp\\ipykernel_33060\\2160885869.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  items = str(row[1][0])[1:-1].replace(\"'\", \"\").replace(\",\", \"\").split(\" \")\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy, precision, recall and f1 for each rule in mig1\n",
    "total = df5_val.shape[0]\n",
    "total_mig = sum(df5_val['MIGRATE5'] == 1)\n",
    "\n",
    "acc_list = []\n",
    "prec_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "for row in mig5_rules.iterrows():\n",
    "    items = str(row[1][0])[1:-1].replace(\"'\", \"\").replace(\",\", \"\").split(\" \")\n",
    "    full_items = items + ['MIGRATE5']\n",
    "    TP = np.sum(np.prod([df5_val[col] for col in full_items], axis=0))\n",
    "    total_rule = np.sum(np.prod([df5_val[col] for col in items], axis=0))\n",
    "    FN = total_mig - TP\n",
    "    FP = total_rule - TP\n",
    "    TN = total - TP - FN - FP\n",
    "\n",
    "    acc_list.append((TP + TN) / (total))\n",
    "    prec_list.append(TP / (TP + FP))\n",
    "    recall_list.append(TP / (TP + FN))\n",
    "    f1_list.append((2 * TP) / (2 * TP + FP + FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7223cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mig5_rules['accuracy'] = acc_list\n",
    "mig5_rules['precision'] = prec_list\n",
    "mig5_rules['recall'] = recall_list\n",
    "mig5_rules['f1_score'] = f1_list\n",
    "mig5_rules.to_csv(f'mig5_final_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37bd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accruacy 0.8413736887669631\n",
      "precision 0.02391112472737087\n",
      "recall 0.3106520090978014\n",
      "f1 0.04440440792993911\n"
     ]
    }
   ],
   "source": [
    "# Final metrics with test set - mig 1\n",
    "total = df1_test.shape[0]\n",
    "total_mig = sum(df1_test['MIGRATE1'] == 1)\n",
    "\n",
    "items = ['LIT', 'NCHILD_2', 'URBAN']\n",
    "full_items = items + ['MIGRATE1']\n",
    "TP = np.sum(np.prod([df1_test[col] for col in full_items], axis=0))\n",
    "total_rule = np.sum(np.prod([df1_test[col] for col in items], axis=0))\n",
    "FN = total_mig - TP\n",
    "FP = total_rule - TP\n",
    "TN = total - TP - FN - FP\n",
    "\n",
    "print(\"accruacy\", (TP + TN) / (total))\n",
    "print(\"precision\", (TP / (TP + FP)))\n",
    "print(\"recall\", (TP / (TP + FN)))\n",
    "print(\"f1\", (2 * TP) / (2 * TP + FP + FN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ddcea7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5791988287634106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_true = df1_test['MIGRATE1']\n",
    "items = ['LIT', 'NCHILD_2', 'URBAN']\n",
    "y_scores = np.prod([df1_test[col] for col in items], axis=0)\n",
    "auc_score = roc_auc_score(y_true, y_scores)\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "876c5c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accruacy 0.7963711460681385\n",
      "precision 0.06476826103077493\n",
      "recall 0.2526979718195759\n",
      "f1 0.10310897038597983\n"
     ]
    }
   ],
   "source": [
    "# Final metrics with test set - mig 5\n",
    "total = df5_test.shape[0]\n",
    "total_mig = sum(df5_test['MIGRATE5'] == 1)\n",
    "\n",
    "items = ['LIT', 'NCHILD_2', 'URBAN']\n",
    "full_items = items + ['MIGRATE5']\n",
    "TP = np.sum(np.prod([df5_test[col] for col in full_items], axis=0))\n",
    "total_rule = np.sum(np.prod([df5_test[col] for col in items], axis=0))\n",
    "FN = total_mig - TP\n",
    "FP = total_rule - TP\n",
    "TN = total - TP - FN - FP\n",
    "\n",
    "print(\"accruacy\", (TP + TN) / (total))\n",
    "print(\"precision\", (TP / (TP + FP)))\n",
    "print(\"recall\", (TP / (TP + FN)))\n",
    "print(\"f1\", (2 * TP) / (2 * TP + FP + FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "197e8dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5377374598629907\n"
     ]
    }
   ],
   "source": [
    "y_true = df5_test['MIGRATE5']\n",
    "items = ['LIT', 'NCHILD_2', 'URBAN']\n",
    "y_scores = np.prod([df5_test[col] for col in items], axis=0)\n",
    "auc_score = roc_auc_score(y_true, y_scores)\n",
    "print(auc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
