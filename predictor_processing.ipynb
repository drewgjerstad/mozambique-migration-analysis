{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3663c930",
   "metadata": {},
   "source": [
    "# Predictor Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2bc12a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Dependencies\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "057c9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Custom Scripts\n",
    "from src.utils.ipums_extract import (\n",
    "    get_ipums_data,\n",
    "    load_ipums_from_pkl,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3994ab77",
   "metadata": {},
   "source": [
    "## New Data Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ad117639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters (do not push your API key to VCS)\n",
    "API_KEY = \"key\"\n",
    "DOWNLOAD_DIR = Path(r\"data\")\n",
    "PKL_EXPORT = True\n",
    "PKL_PATH = Path(r\"data/ipums_extract.pkl\")\n",
    "\n",
    "collection = \"ipumsi\"\n",
    "description = \"data mining mozambique project\"\n",
    "samples = [\"mz1997a\", \"mz2007a\", \"mz2017a\"]\n",
    "\n",
    "variables = ['PERSONS', 'GQ', 'URBAN', 'GEO1_MZ', 'GEO2_MZ', 'OWNERSHIP',\n",
    "             'PHONE', 'AUTOS', 'ROOMS', 'HHTYPE', 'RESIDENT', 'FAMSIZE',\n",
    "             'NCHILD', 'AGE', 'SEX', 'MARST', 'MORTMOT', 'MORTFAT',\n",
    "             'NATIVITY', 'CITIZEN', 'BPL1_MZ', 'SCHOOL', 'LIT',\n",
    "             'EDATTAIN', 'EMPSTAT', 'LABFORCE', 'MIGRATE1', 'MIGRATE5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a61fdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract submitted to IPUMS. Extract ID: 6.\n",
      "Waiting for extract to finish processing on IPUMS server...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get IPUMS Data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m ipums_df = \u001b[43mget_ipums_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m=\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAPI_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDOWNLOAD_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpkl_export\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPKL_EXPORT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpkl_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPKL_PATH\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CSCI 5523/umn-fall2025-csci5523-project/src/utils/ipums_extract.py:74\u001b[39m, in \u001b[36mget_ipums_data\u001b[39m\u001b[34m(collection, description, samples, variables, api_key, download_dir, pkl_export, pkl_path)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Wait for Extract\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWaiting for extract to finish processing on IPUMS server...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[43mipums\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_for_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Download Extract\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading extract to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownload_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ipumspy/api/core.py:396\u001b[39m, in \u001b[36mIpumsApiClient.wait_for_extract\u001b[39m\u001b[34m(self, extract, collection, inital_wait_time, max_wait_time, timeout)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total_time >= timeout:\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IpumsTimeoutException(\n\u001b[32m    392\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMore than \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds have passed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    393\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhile waiting for your extract to finish building\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    394\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m status = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m status == \u001b[33m\"\u001b[39m\u001b[33mfailed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    398\u001b[39m     \u001b[38;5;66;03m# TODO: follow up with IT and user support to see if we should\u001b[39;00m\n\u001b[32m    399\u001b[39m     \u001b[38;5;66;03m# instruct people to email us about extract failures.\u001b[39;00m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IpumsExtractFailure(\n\u001b[32m    401\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOops! Your \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m extract number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    402\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto complete.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    403\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ipumspy/api/core.py:221\u001b[39m, in \u001b[36mIpumsApiClient.extract_status\u001b[39m\u001b[34m(self, extract, collection)\u001b[39m\n\u001b[32m    218\u001b[39m extract_id, collection = _extract_and_collection(extract, collection)\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/extracts/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mextract_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcollection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mversion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_version\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m IpumsNotFound:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnot found\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ipumspy/api/core.py:158\u001b[39m, in \u001b[36mIpumsApiClient.get\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs) -> requests.Response:\n\u001b[32m    157\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"GET a request from the IPUMS API\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ipumspy/api/core.py:52\u001b[39m, in \u001b[36mretry_on_transient_error.<locals>.wrapped_func\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_retries - \u001b[32m1\u001b[39m):\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TransientIpumsApiException:\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ipumspy/api/core.py:123\u001b[39m, in \u001b[36mIpumsApiClient.request\u001b[39m\u001b[34m(self, method, *args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[33;03mSubmit a request to the IPUMS API\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     response.raise_for_status()\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Get IPUMS Data\n",
    "ipums_df = get_ipums_data(\n",
    "    collection=collection,\n",
    "    description=description,\n",
    "    samples=samples,\n",
    "    variables=variables,\n",
    "    api_key=API_KEY,\n",
    "    download_dir=DOWNLOAD_DIR,\n",
    "    pkl_export=PKL_EXPORT,\n",
    "    pkl_path=PKL_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "daf2e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data from PKL\n",
    "ipums_df_pkl = load_ipums_from_pkl(PKL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9d56737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to fix NIU, unknown and other issues\n",
    "var_dicts = ''\n",
    "with open('var_dictionaries.txt', encoding='utf-8') as f:\n",
    "    exec(\"var_dicts = \" + f.read())\n",
    "\n",
    "for v in var_dicts.keys():\n",
    "    if v in ipums_df_pkl.columns:\n",
    "        ipums_df_pkl[v] = ipums_df_pkl[v].map(lambda x: var_dicts[v].get(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad34ad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Migration NIUS and unknown\n",
    "mig1_data = ipums_df_pkl[~ipums_df_pkl['MIGRATE1'].isna()].copy()\n",
    "mig5_data = ipums_df_pkl[~ipums_df_pkl['MIGRATE5'].isna()].copy()\n",
    "\n",
    "# Make new variance for prediction\n",
    "mig1_data['mig_provincial'] = mig1_data['MIGRATE1']\n",
    "mig5_data['mig_provincial'] = mig5_data['MIGRATE5']\n",
    "\n",
    "# Rename columns\n",
    "mig1_data = mig1_data.drop(['MIGRATE1', 'MIGRATE5'], axis=1)\n",
    "mig5_data = mig5_data.drop(['MIGRATE1', 'MIGRATE5'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc30bf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mig_provincial\n",
      "0.0    5860462\n",
      "1.0      69067\n",
      "Name: count, dtype: int64\n",
      "mig_provincial\n",
      "0.0    4746396\n",
      "1.0     228173\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(mig1_data['mig_provincial'].value_counts(dropna=False))\n",
    "print(mig5_data['mig_provincial'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a47bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Metadata Columns\n",
    "metadata_cols = ['COUNTRY', 'SAMPLE', 'SERIAL', 'HHWT', 'PERNUM', 'PERWT']\n",
    "mig1_data.drop(columns=metadata_cols, inplace=True)\n",
    "mig5_data.drop(columns=metadata_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93efef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Detailed Columns\n",
    "detailed_cols = ['OWNERSHIPD', 'MARSTD', 'EDATTAIND', 'EMPSTATD',\n",
    "                 'GEO1_MZ', 'GEO2_MZ']\n",
    "mig1_data.drop(columns=detailed_cols, inplace=True)\n",
    "mig5_data.drop(columns=detailed_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48b335e",
   "metadata": {},
   "source": [
    "## Predictor conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a43a4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YEAR: 3 variables, (0 or 1) for [1997, 2007, 2017]\n",
    "# var name: YEAR_<YEAR> (ex: YEAR_1997)\n",
    "\n",
    "# Create New Columns (both mig1 and mig5!)\n",
    "mig1_data['YEAR_1997'] = np.where(mig1_data['YEAR'] == '1997', 1, 0)\n",
    "mig1_data['YEAR_2007'] = np.where(mig1_data['YEAR'] == '2007', 1, 0)\n",
    "mig1_data['YEAR_2017'] = np.where(mig1_data['YEAR'] == '2017', 1, 0)\n",
    "\n",
    "mig5_data['YEAR_1997'] = np.where(mig5_data['YEAR'] == '1997', 1, 0)\n",
    "mig5_data['YEAR_2007'] = np.where(mig5_data['YEAR'] == '2007', 1, 0)\n",
    "mig5_data['YEAR_2017'] = np.where(mig5_data['YEAR'] == '2017', 1, 0)\n",
    "\n",
    "# Drop Old Column\n",
    "mig1_data.drop(columns=['YEAR'], inplace=True)\n",
    "mig5_data.drop(columns=['YEAR'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d2761",
   "metadata": {},
   "source": [
    "For binning, use `unique()` (i.e., `mig1_data['PERSONS'].unique()`) to identify\n",
    "the unique values and always leave space (if necessary/reasonable) on top for\n",
    "generality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "550990aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERSONS: binning (by 10s?) [should be about 5 variables]\n",
    "# var name: PERSONS_<cutoff> (ex: 10 and below -> PERSONS_10)\n",
    "\n",
    "# Create New Columns (both mig1 and mig5!)\n",
    "mig1_data['PERSONS_10'] = np.where(mig1_data['PERSONS'] <= 10, 1, 0)                                      # PERSONS ≤ 10\n",
    "mig1_data['PERSONS_20'] = np.where((mig1_data['PERSONS'] >= 11) & (mig1_data['PERSONS'] <= 20), 1, 0)     # 11 ≤ PERSONS ≤ 20\n",
    "mig1_data['PERSONS_30'] = np.where((mig1_data['PERSONS'] >= 21) & (mig1_data['PERSONS'] <= 30), 1, 0)     # 21 ≤ PERSONS ≤ 30\n",
    "mig1_data['PERSONS_40'] = np.where((mig1_data['PERSONS'] >= 31) & (mig1_data['PERSONS'] <= 40), 1, 0)     # 31 ≤ PERSONS ≤ 40\n",
    "mig1_data['PERSONS_50'] = np.where((mig1_data['PERSONS'] >= 41) & (mig1_data['PERSONS'] <= 50), 1, 0)     # 41 ≤ PERSONS ≤ 50\n",
    "\n",
    "mig5_data['PERSONS_10'] = np.where(mig5_data['PERSONS'] <= 10, 1, 0)                                      # PERSONS ≤ 10\n",
    "mig5_data['PERSONS_20'] = np.where((mig5_data['PERSONS'] >= 11) & (mig5_data['PERSONS'] <= 20), 1, 0)     # 11 ≤ PERSONS ≤ 20\n",
    "mig5_data['PERSONS_30'] = np.where((mig5_data['PERSONS'] >= 21) & (mig5_data['PERSONS'] <= 30), 1, 0)     # 21 ≤ PERSONS ≤ 30\n",
    "mig5_data['PERSONS_40'] = np.where((mig5_data['PERSONS'] >= 31) & (mig5_data['PERSONS'] <= 40), 1, 0)     # 31 ≤ PERSONS ≤ 40\n",
    "mig5_data['PERSONS_50'] = np.where((mig5_data['PERSONS'] >= 41) & (mig5_data['PERSONS'] <= 50), 1, 0)     # 41 ≤ PERSONS ≤ 50\n",
    "\n",
    "# Drop Old Column\n",
    "mig1_data.drop(columns=['PERSONS'], inplace=True)\n",
    "mig5_data.drop(columns=['PERSONS'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb852f",
   "metadata": {},
   "source": [
    "Special case for combining several categories. Once again, `unique()` will be\n",
    "useful. Note that we use `|` for `or` just as we did above, using `&` for `and`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19407ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GQ: 3 variables for [Household, Institution, Other]\n",
    "# var name: GQ_<type> (ex: GQ_HOUSEHOLD)\n",
    "\n",
    "# Create New Columns (both mig1 and mig5!)\n",
    "mig1_data['GQ_HOUSEHOLD'] = np.where(mig1_data['GQ'] == 'Households', 1, 0)\n",
    "mig1_data['GQ_INSTITUTION'] = np.where(mig1_data['GQ'] == 'Institutions', 1, 0)\n",
    "mig1_data['GQ_OTHER'] = np.where((mig1_data['GQ'] == '1-person unit created by splitting large household') |\n",
    "                                 (mig1_data['GQ'] == 'Other group quarters'), 1, 0)\n",
    "\n",
    "mig5_data['GQ_HOUSEHOLD'] = np.where(mig5_data['GQ'] == 'Households', 1, 0)\n",
    "mig5_data['GQ_INSTITUTION'] = np.where(mig5_data['GQ'] == 'Institutions', 1, 0)\n",
    "mig5_data['GQ_OTHER'] = np.where((mig5_data['GQ'] == '1-person unit created by splitting large household') |\n",
    "                                 (mig5_data['GQ'] == 'Other group quarters'), 1, 0)\n",
    "\n",
    "# Drop Old Column\n",
    "mig1_data.drop(columns=['GQ'], inplace=True)\n",
    "mig5_data.drop(columns=['GQ'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d4106",
   "metadata": {},
   "source": [
    "Use `.fillna(0)` to convert `nan` to `0`. Then use `.astype(int)` to convert\n",
    "to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "570139b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URBAN (already binary, convert to int)\n",
    "mig1_data['URBAN']= mig1_data['URBAN'].fillna(0).astype(int)\n",
    "mig5_data['URBAN'] = mig5_data['URBAN'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66a22c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OWNERSHIP (already binary, convert to int)\n",
    "mig1_data['OWNERSHIP'] = mig1_data['OWNERSHIP'].fillna(0).astype(int)\n",
    "mig5_data['OWNERSHIP'] = mig5_data['OWNERSHIP'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fc6297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHONE (already binary, convert to int)\n",
    "mig1_data['PHONE'] = mig1_data['PHONE'].fillna(0).astype(int)\n",
    "mig5_data['PHONE'] = mig5_data['PHONE'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "947b6adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOS (already binary, convert to int)\n",
    "mig1_data['AUTOS'] = mig1_data['AUTOS'].fillna(0).astype(int)\n",
    "mig5_data['AUTOS'] = mig5_data['AUTOS'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "686f0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOMS: similar to PERSONS (bin by 5s)\n",
    "# var name: ROOMS_<cutoff> (ex: 5 and below -> ROOMS_5)\n",
    "\n",
    "# Create New mig1 column \n",
    "mig1_data['ROOMS_5'] = np.where(mig1_data['ROOMS'] <= 5, 1, 0)                                      # ROOMS ≤ 5\n",
    "mig1_data['ROOMS_10'] = np.where((mig1_data['ROOMS'] >= 6) & (mig1_data['ROOMS'] <= 10), 1, 0)     # 6 ≤ ROOMS ≤ 10\n",
    "\n",
    "#might be redundant, not all of the data sets go this high\n",
    "mig1_data['ROOMS_15'] = np.where((mig1_data['ROOMS'] >= 11) & (mig1_data['ROOMS'] <= 15), 1, 0)     # 11 ≤ ROOMS ≤ 15\n",
    "mig1_data['ROOMS_20'] = np.where((mig1_data['ROOMS'] >= 11) & (mig1_data['ROOMS'] <= 15), 1, 0)     # 16 ≤ ROOMS ≤ 20\n",
    "\n",
    "\n",
    "\n",
    "#Create new mig5 column\n",
    "mig5_data['ROOMS_5'] = np.where(mig5_data['ROOMS'] <= 5, 1, 0)                                      # ROOMS ≤ 5\n",
    "mig5_data['ROOMS_10'] = np.where((mig5_data['ROOMS'] >= 6) & (mig5_data['ROOMS'] <= 10), 1, 0)     # 6 ≤ ROOMS ≤ 10\n",
    "\n",
    "#might be redundant, not all of the data sets go this high\n",
    "mig5_data['ROOMS_15'] = np.where((mig5_data['ROOMS'] >= 11) & (mig5_data['ROOMS'] <= 15), 1, 0)     # 11 ≤ ROOMS ≤ 15\n",
    "mig5_data['ROOMS_20'] = np.where((mig5_data['ROOMS'] >= 11) & (mig5_data['ROOMS'] <= 15), 1, 0)     # 16 ≤ ROOMS ≤ 20\n",
    "\n",
    "\n",
    "\n",
    "# Drop Old Column\n",
    "mig1_data.drop(columns=['ROOMS'], inplace=True)\n",
    "mig5_data.drop(columns=['ROOMS'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8bfa0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HHTYPE: combine as follows\n",
    "#   HHTYPE_MARRIED: 'Married/cohab couple with children', 'Married/cohab couple, no children'\n",
    "#   HHTYPE_SINGLE: 'Single-parent family', 'One-person household'\n",
    "#   HHTYPE_OTHER: the remaining categories\n",
    "\n",
    "# defining data into easier variables to use\n",
    "mig1_married = mig1_data['HHTYPE'].isin([\n",
    "    'Married/cohab couple with children',\n",
    "    'Married/cohab couple, no children'\n",
    "])\n",
    "mig1_single = mig1_data['HHTYPE'].isin([\n",
    "    'Single-parent family',\n",
    "    'One-person household'\n",
    "])\n",
    "\n",
    "mig5_married = mig5_data['HHTYPE'].isin([\n",
    "    'Married/cohab couple with children',\n",
    "    'Married/cohab couple, no children'\n",
    "])\n",
    "mig5_single = mig5_data['HHTYPE'].isin([\n",
    "    'Single-parent family',\n",
    "    'One-person household'\n",
    "])\n",
    "\n",
    "\n",
    "# Create New Columns (both mig1 and mig5!)\n",
    "mig1_data['HHTYPE_MARRIED'] = np.where(mig1_married, 1, 0) # HHTYPE_MARRIED: 'Married/cohab couple with children', 'Married/cohab couple, no children'\n",
    "mig1_data['HHTYPE_SINGLE'] = np.where(mig1_single, 1, 0) # HHTYPE_SINGLE: 'Single-parent family', 'One-person household'\n",
    "mig1_data['HHTYPE_OTHER'] = np.where(~(mig1_married | mig1_single), 1, 0) # HHTYPE_OTHER: the remaining categories\n",
    "\n",
    "mig5_data['HHTYPE_MARRIED'] = np.where(mig5_married, 1, 0) # HHTYPE_MARRIED: 'Married/cohab couple with children', 'Married/cohab couple, no children'\n",
    "mig5_data['HHTYPE_SINGLE'] = np.where(mig5_single, 1, 0) # HHTYPE_SINGLE: 'Single-parent family', 'One-person household'\n",
    "mig5_data['HHTYPE_OTHER'] = np.where(~(mig5_married | mig5_single), 1, 0) # HHTYPE_OTHER: the remaining categories\n",
    "\n",
    "\n",
    "# Drop Old Column\n",
    "mig1_data.drop(columns=['HHTYPE'], inplace=True)\n",
    "mig5_data.drop(columns=['HHTYPE'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83979071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESIDENT: 1 for present, 0 for absent (only conversion, no expansion needed)\n",
    "mig1_data['RESIDENT'] = np.where(\n",
    "    (mig1_data['RESIDENT'] == 'Present resident') |\n",
    "    (mig1_data['RESIDENT'] == 'Absent resident'), 1, 0 )\n",
    "\n",
    "mig5_data['RESIDENT'] = np.where(\n",
    "    (mig5_data['RESIDENT'] == 'Present resident') |\n",
    "    (mig5_data['RESIDENT'] == 'Absent resident'), 1, 0 )\n",
    "\n",
    "mig1_data.drop(columns=['RESIDENT'], inplace=True)\n",
    "mig5_data.drop(columns=['RESIDENT'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c1c7fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAMSIZE: similar to PERSONS (bin by 5s)\n",
    "# var name: FAMSIZE_<cutoff> (ex: 5 and below -> FAMSIZE_5)\n",
    "\n",
    "# Create New mig1 column \n",
    "mig1_data['FAMSIZE_5'] = np.where(mig1_data['FAMSIZE'] <= 5, 1, 0)                                      # FAMSIZE ≤ 5\n",
    "mig1_data['FAMSIZE_10'] = np.where((mig1_data['FAMSIZE'] >= 6) & (mig1_data['FAMSIZE'] <= 10), 1, 0)     # 6 ≤ FAMSIZE ≤ 10\n",
    "mig1_data['FAMSIZE_15'] = np.where((mig1_data['FAMSIZE'] >= 11) & (mig1_data['FAMSIZE'] <= 15), 1, 0)     # 11 ≤ FAMSIZE ≤ 15\n",
    "mig1_data['FAMSIZE_20'] = np.where((mig1_data['FAMSIZE'] >= 16) & (mig1_data['FAMSIZE'] <= 20), 1, 0)     # 16 ≤ FAMSIZE ≤ 20\n",
    "mig1_data['FAMSIZE_25'] = np.where((mig1_data['FAMSIZE'] >= 21) & (mig1_data['FAMSIZE'] <= 25), 1, 0)     # 21 ≤ FAMSIZE ≤ 25\n",
    "mig1_data['FAMSIZE_30'] = np.where((mig1_data['FAMSIZE'] >= 26) & (mig1_data['FAMSIZE'] <= 30), 1, 0)     # 26 ≤ FAMSIZE ≤ 30\n",
    "mig1_data['FAMSIZE_35'] = np.where((mig1_data['FAMSIZE'] >= 31) & (mig1_data['FAMSIZE'] <= 35), 1, 0)     # 31 ≤ FAMSIZE ≤ 35\n",
    "\n",
    "#might be redundant, not all of the data sets go this high\n",
    "mig1_data['FAMSIZE_40'] = np.where((mig1_data['FAMSIZE'] >= 36) & (mig1_data['FAMSIZE'] <= 40), 1, 0)     # 36 ≤ FAMSIZE ≤ 40\n",
    "mig1_data['FAMSIZE_45'] = np.where((mig1_data['FAMSIZE'] >= 41) & (mig1_data['FAMSIZE'] <= 45), 1, 0)     # 41 ≤ FAMSIZE ≤ 45\n",
    "\n",
    "# new mig5 columns\n",
    "mig5_data['FAMSIZE_5'] = np.where(mig5_data['FAMSIZE'] <= 5, 1, 0)                                      # FAMSIZE ≤ 5\n",
    "mig5_data['FAMSIZE_10'] = np.where((mig5_data['FAMSIZE'] >= 6) & (mig5_data['FAMSIZE'] <= 10), 1, 0)     # 6 ≤ FAMSIZE ≤ 10\n",
    "mig5_data['FAMSIZE_15'] = np.where((mig5_data['FAMSIZE'] >= 11) & (mig5_data['FAMSIZE'] <= 15), 1, 0)     # 11 ≤ FAMSIZE ≤ 15\n",
    "mig5_data['FAMSIZE_20'] = np.where((mig5_data['FAMSIZE'] >= 16) & (mig5_data['FAMSIZE'] <= 20), 1, 0)     # 16 ≤ FAMSIZE ≤ 20\n",
    "mig5_data['FAMSIZE_25'] = np.where((mig5_data['FAMSIZE'] >= 21) & (mig5_data['FAMSIZE'] <= 25), 1, 0)     # 21 ≤ FAMSIZE ≤ 25\n",
    "mig5_data['FAMSIZE_30'] = np.where((mig5_data['FAMSIZE'] >= 26) & (mig5_data['FAMSIZE'] <= 30), 1, 0)     # 26 ≤ FAMSIZE ≤ 30\n",
    "mig5_data['FAMSIZE_35'] = np.where((mig5_data['FAMSIZE'] >= 31) & (mig5_data['FAMSIZE'] <= 35), 1, 0)     # 31 ≤ FAMSIZE ≤ 35\n",
    "\n",
    "#might be redundant, not all of the data sets go this high\n",
    "mig5_data['FAMSIZE_40'] = np.where((mig5_data['FAMSIZE'] >= 36) & (mig5_data['FAMSIZE'] <= 40), 1, 0)     # 36 ≤ FAMSIZE ≤ 40\n",
    "mig5_data['FAMSIZE_45'] = np.where((mig5_data['FAMSIZE'] >= 41) & (mig5_data['FAMSIZE'] <= 45), 1, 0)     # 41 ≤ FAMSIZE ≤ 45\n",
    "\n",
    "\n",
    "# Drop Old Column\n",
    "mig1_data.drop(columns=['FAMSIZE'], inplace=True)\n",
    "mig5_data.drop(columns=['FAMSIZE'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "46480bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCHILD: similar to PERSONS (bin by 2s)\n",
    "# var name: NCHILD_<cutoff> (ex: 2 and below -> NCHILD_2)\n",
    "\n",
    "# Create New mig1 column \n",
    "mig1_data['NCHILD_2'] = np.where(mig1_data['NCHILD'] <= 2, 1, 0)                                      # NCHILD ≤ 2\n",
    "mig1_data['NCHILD_4'] = np.where((mig1_data['NCHILD'] >= 3) & (mig1_data['NCHILD'] <= 4), 1, 0)     # 3 ≤ NCHILD ≤ 4\n",
    "mig1_data['NCHILD_6'] = np.where((mig1_data['NCHILD'] >= 5) & (mig1_data['NCHILD'] <= 6), 1, 0)     # 5 ≤ NCHILD ≤ 6\n",
    "mig1_data['NCHILD_8'] = np.where((mig1_data['NCHILD'] >= 7) & (mig1_data['NCHILD'] <= 8), 1, 0)     # 7 ≤ NCHILD ≤ 8\n",
    "mig1_data['NCHILD_10'] = np.where((mig1_data['NCHILD'] >= 9) & (mig1_data['NCHILD'] <= 10), 1, 0)     # 9 ≤ NCHILD ≤ 10\n",
    "\n",
    "\n",
    "#Create new mig5 column\n",
    "\n",
    "mig5_data['NCHILD_2'] = np.where(mig5_data['NCHILD'] <= 2, 1, 0)                                      # NCHILD ≤ 2\n",
    "mig5_data['NCHILD_4'] = np.where((mig5_data['NCHILD'] >= 3) & (mig5_data['NCHILD'] <= 4), 1, 0)     # 3 ≤ NCHILD ≤ 4\n",
    "mig5_data['NCHILD_6'] = np.where((mig5_data['NCHILD'] >= 5) & (mig5_data['NCHILD'] <= 6), 1, 0)     # 5 ≤ NCHILD ≤ 6\n",
    "mig5_data['NCHILD_8'] = np.where((mig5_data['NCHILD'] >= 7) & (mig5_data['NCHILD'] <= 8), 1, 0)     # 7 ≤ NCHILD ≤ 8\n",
    "mig5_data['NCHILD_10'] = np.where((mig5_data['NCHILD'] >= 9) & (mig5_data['NCHILD'] <= 10), 1, 0)     # 9 ≤ NCHILD ≤ 10\n",
    "\n",
    "\n",
    "# Drop Old Column\n",
    "mig1_data.drop(columns=['NCHILD'], inplace=True)\n",
    "mig5_data.drop(columns=['NCHILD'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8739078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGE: similar to PERSONS (bin by 10s) -> all the way to 100\n",
    "# varname: AGE_<cutoff> (ex: 10 and below -> AGE_10)\n",
    "\n",
    "# Create New mig1 column \n",
    "mig1_data['AGE_10'] = np.where(mig1_data['AGE'] <= 10, 1, 0)                                      # AGE ≤ 10\n",
    "mig1_data['AGE_20'] = np.where((mig1_data['AGE'] >= 11) & (mig1_data['AGE'] <= 20), 1, 0)     # 11 ≤ AGE ≤ 20\n",
    "mig1_data['AGE_30'] = np.where((mig1_data['AGE'] >= 21) & (mig1_data['AGE'] <= 30), 1, 0)     # 21 ≤ AGE ≤ 30\n",
    "mig1_data['AGE_40'] = np.where((mig1_data['AGE'] >= 31) & (mig1_data['AGE'] <= 40), 1, 0)     # 31 ≤ AGE ≤ 40\n",
    "mig1_data['AGE_50'] = np.where((mig1_data['AGE'] >= 41) & (mig1_data['AGE'] <= 50), 1, 0)     # 41 ≤ AGE ≤ 50\n",
    "mig1_data['AGE_60'] = np.where((mig1_data['AGE'] >= 51) & (mig1_data['AGE'] <= 60), 1, 0)     # 51 ≤ AGE ≤ 60\n",
    "mig1_data['AGE_70'] = np.where((mig1_data['AGE'] >= 61) & (mig1_data['AGE'] <= 70), 1, 0)     # 61 ≤ AGE ≤ 70\n",
    "mig1_data['AGE_80'] = np.where((mig1_data['AGE'] >= 71) & (mig1_data['AGE'] <= 80), 1, 0)     # 71 ≤ AGE ≤ 80\n",
    "mig1_data['AGE_90'] = np.where((mig1_data['AGE'] >= 81) & (mig1_data['AGE'] <= 90), 1, 0)     # 81 ≤ AGE ≤ 90\n",
    "mig1_data['AGE_100'] = np.where((mig1_data['AGE'] >= 91) & (mig1_data['AGE'] <= 100), 1, 0)     # 91 ≤ AGE ≤ 100\n",
    "\n",
    "# Create New mig5 column \n",
    "mig5_data['AGE_10'] = np.where(mig5_data['AGE'] <= 10, 1, 0)                                      # AGE ≤ 10\n",
    "mig5_data['AGE_20'] = np.where((mig5_data['AGE'] >= 11) & (mig5_data['AGE'] <= 20), 1, 0)     # 11 ≤ AGE ≤ 20\n",
    "mig5_data['AGE_30'] = np.where((mig5_data['AGE'] >= 21) & (mig5_data['AGE'] <= 30), 1, 0)     # 21 ≤ AGE ≤ 30\n",
    "mig5_data['AGE_40'] = np.where((mig5_data['AGE'] >= 31) & (mig5_data['AGE'] <= 40), 1, 0)     # 31 ≤ AGE ≤ 40\n",
    "mig5_data['AGE_50'] = np.where((mig5_data['AGE'] >= 41) & (mig5_data['AGE'] <= 50), 1, 0)     # 41 ≤ AGE ≤ 50\n",
    "mig5_data['AGE_60'] = np.where((mig5_data['AGE'] >= 51) & (mig5_data['AGE'] <= 60), 1, 0)     # 51 ≤ AGE ≤ 60\n",
    "mig5_data['AGE_70'] = np.where((mig5_data['AGE'] >= 61) & (mig5_data['AGE'] <= 70), 1, 0)     # 61 ≤ AGE ≤ 70\n",
    "mig5_data['AGE_80'] = np.where((mig5_data['AGE'] >= 71) & (mig5_data['AGE'] <= 80), 1, 0)     # 71 ≤ AGE ≤ 80\n",
    "mig5_data['AGE_90'] = np.where((mig5_data['AGE'] >= 81) & (mig5_data['AGE'] <= 90), 1, 0)     # 81 ≤ AGE ≤ 90\n",
    "mig5_data['AGE_100'] = np.where((mig5_data['AGE'] >= 91) & (mig5_data['AGE'] <= 100), 1, 0)     # 91 ≤ AGE ≤ 100\n",
    "\n",
    "# Drop Old Column\n",
    "mig1_data.drop(columns=['AGE'], inplace=True)\n",
    "mig5_data.drop(columns=['AGE'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e65c22bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARST: 4 variables\n",
    "# categories:\n",
    "#    - 'MARRIED': 'Married/in union'\n",
    "#    - 'SINGLE': 'Single/never married'\n",
    "#    - 'OTHER': 'Separated...' or 'Widowed'\n",
    "# varname: MARST_<category> (ex: MARST_MARRIED)\n",
    "\n",
    "# variables for each group on mig1_data\n",
    "mig1_married = mig1_data['MARST'] == 'Married/in union'\n",
    "mig1_single = mig1_data['MARST'] == 'Single/never married'\n",
    "mig1_other = (mig1_data['MARST'] == 'Separated/divorced/spouse absent') | (mig1_data['MARST'] == 'Widowed')\n",
    "\n",
    "#  same for mig5_data\n",
    "mig5_married = mig5_data['MARST'] == 'Married/in union'\n",
    "mig5_single = mig5_data['MARST'] == 'Single/never married'\n",
    "mig5_other = (mig5_data['MARST'] == 'Separated/divorced/spouse absent') | (mig5_data['MARST'] == 'Widowed')\n",
    "\n",
    "# Create New Columns (both mig1 and mig5!)\n",
    "mig1_data['MARST_MARRIED'] = np.where(mig1_married, 1, 0)\n",
    "mig1_data['MARST_SINGLE'] = np.where(mig1_single, 1, 0)\n",
    "mig1_data['MARST_OTHER'] = np.where(mig1_other, 1, 0)\n",
    "\n",
    "mig5_data['MARST_MARRIED'] = np.where(mig5_married, 1, 0)\n",
    "mig5_data['MARST_SINGLE'] = np.where(mig5_single, 1, 0)\n",
    "mig5_data['MARST_OTHER'] = np.where(mig5_other, 1, 0)\n",
    "\n",
    "# Drop Old Column\n",
    "mig1_data.drop(columns=['MARST'], inplace=True)\n",
    "mig5_data.drop(columns=['MARST'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51ed120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MORTMOT (already in binary, convert to int)\n",
    "mig1_data['MORTMOT'] = mig1_data['MORTMOT'].fillna(0).astype(int)\n",
    "mig5_data['MORTMOT'] = mig5_data['MORTMOT'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09036b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MORTFAT (already in binary, convert to int)\n",
    "mig1_data['MORTFAT'] = mig1_data['MORTFAT'].fillna(0).astype(int)\n",
    "mig5_data['MORTFAT'] = mig5_data['MORTFAT'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "466932b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NATIVITY (already in binary, convert to int)\n",
    "mig1_data['NATIVITY'] = mig1_data['NATIVITY'].fillna(0).astype(int)\n",
    "mig5_data['NATIVITY'] = mig5_data['NATIVITY'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4efc109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CITIZEN (already in history, convert to int)\n",
    "mig1_data['CITIZEN'] = mig1_data['CITIZEN'].fillna(0).astype(int)\n",
    "mig5_data['CITIZEN'] = mig5_data['CITIZEN'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5eeca062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPL1_MZ: 2 variables\n",
    "#   categories\n",
    "#     - 'FOREIGN': 'Foreign Country'\n",
    "#     - 'DOMESTIC': all others\n",
    "#   var name: BP_<category> (ex: BP_FOREIGN)\n",
    "\n",
    "# variables on mig1_data\n",
    "mig1_foreign = mig1_data['BPL1_MZ'] == 'Foreign Country'\n",
    "mig1_domestic = mig1_data['BPL1_MZ'] != 'Foreign Country'\n",
    "\n",
    "#mig5_data\n",
    "mig5_foreign = mig5_data['BPL1_MZ'] == 'Foreign Country'\n",
    "mig5_domestic = mig5_data['BPL1_MZ'] != 'Foreign Country'\n",
    "\n",
    "# Create New Columns (both mig1 and mig5!)\n",
    "mig1_data['BP_FOREIGN'] = np.where(mig1_foreign, 1, 0)\n",
    "mig1_data['BP_DOMESTIC'] = np.where(mig1_domestic, 1, 0)\n",
    "\n",
    "mig5_data['BP_FOREIGN'] = np.where(mig5_foreign, 1, 0)\n",
    "mig5_data['BP_DOMESTIC'] = np.where(mig5_domestic, 1, 0)\n",
    "\n",
    "# Drop Old Column if desired\n",
    "mig1_data.drop(columns=['BPL1_MZ'], inplace=True)\n",
    "mig5_data.drop(columns=['BPL1_MZ'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4b2c1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCHOOL: convert to SCHOOL (Yes=1, No=0) (no expansion, just convert)\n",
    "mig1_data['SCHOOL'] = np.where(mig1_data['SCHOOL'] == 'Yes', 1, 0)\n",
    "mig5_data['SCHOOL'] = np.where(mig5_data['SCHOOL'] == 'Yes', 1, 0)\n",
    "\n",
    "\n",
    "mig1_data.drop(columns=['SCHOOL'], inplace=True)\n",
    "mig5_data.drop(columns=['SCHOOL'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b23d8b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIT (already binary, convert to int)\n",
    "mig1_data['LIT'] = mig1_data['LIT'].fillna(0).astype(int)\n",
    "mig5_data['LIT'] = mig5_data['LIT'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c4443328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDATTAIN: \n",
    "#   categories:\n",
    "#    - 'NONE': 'Less than primary completed'\n",
    "#    - 'PRIMARY': 'Primary completed'\n",
    "#    - 'SECONDARY': 'Secondary completed'\n",
    "#    - 'HIGHER': 'University completed'\n",
    "# var name: EDU_<category> (ex: EDU_NONE)\n",
    "\n",
    "# mig1_data\n",
    "mig1_none = mig1_data['EDATTAIN'] == 'Less than primary completed'\n",
    "mig1_primary = mig1_data['EDATTAIN'] == 'Primary completed'\n",
    "mig1_secondary = mig1_data['EDATTAIN'] == 'Secondary completed'\n",
    "mig1_higher = mig1_data['EDATTAIN'] == 'University completed'\n",
    "\n",
    "# mig5_data\n",
    "mig5_none = mig5_data['EDATTAIN'] == 'Less than primary completed'\n",
    "mig5_primary = mig5_data['EDATTAIN'] == 'Primary completed'\n",
    "mig5_secondary = mig5_data['EDATTAIN'] == 'Secondary completed'\n",
    "mig5_higher = mig5_data['EDATTAIN'] == 'University completed'\n",
    "\n",
    "# Create New Columns (both mig1 and mig5!)\n",
    "mig1_data['EDU_NONE'] = np.where(mig1_none, 1, 0)\n",
    "mig1_data['EDU_PRIMARY'] = np.where(mig1_primary, 1, 0)\n",
    "mig1_data['EDU_SECONDARY'] = np.where(mig1_secondary, 1, 0)\n",
    "mig1_data['EDU_HIGHER'] = np.where(mig1_higher, 1, 0)\n",
    "\n",
    "mig5_data['EDU_NONE'] = np.where(mig5_none, 1, 0)\n",
    "mig5_data['EDU_PRIMARY'] = np.where(mig5_primary, 1, 0)\n",
    "mig5_data['EDU_SECONDARY'] = np.where(mig5_secondary, 1, 0)\n",
    "mig5_data['EDU_HIGHER'] = np.where(mig5_higher, 1, 0)\n",
    "\n",
    "# Drop Old Column if desired\n",
    "mig1_data.drop(columns=['EDATTAIN'], inplace=True)\n",
    "mig5_data.drop(columns=['EDATTAIN'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1ffe4c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMPSTAT: convert to EMPSTAT (Employed=1, Unemployed/Inactive=0)\n",
    "mig1_data['EMPSTAT'] = np.where(\n",
    "    mig1_data['EMPSTAT'] == 'Employed', 1, 0\n",
    ")\n",
    "mig5_data['EMPSTAT'] = np.where(\n",
    "    mig5_data['EMPSTAT'] == 'Employed', 1, 0\n",
    ")\n",
    "\n",
    "mig1_data.drop(columns=['EMPSTAT'], inplace=True)\n",
    "mig5_data.drop(columns=['EMPSTAT'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "76bcfd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABFORCE (already binary, convert to int)\n",
    "mig1_data['LABFORCE'] = mig1_data['LABFORCE'].fillna(0).astype(int)\n",
    "mig5_data['LABFORCE'] = mig5_data['LABFORCE'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e7447f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mig_provincial (rename to MIG, convert to int)\n",
    "mig1_data['MIG'] = mig1_data['mig_provincial'].fillna(0).astype(int)\n",
    "mig5_data['MIG'] = mig5_data['mig_provincial'].fillna(0).astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
